{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62fdef-2b9d-418c-a927-5f3ce047b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b777e0-acd7-4d68-96fd-fdc2967e8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ded95-d8ae-48ef-ac64-c1516a01b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b8e49-70d5-4eb1-affa-305d2860b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feed-forward network helps in forward propagation. At each neuron in a hidden or output layer, the processing happens in two steps: Preactivation: it is a weighted sum of inputs i.e. the linear transformation of weights w.r.t to inputs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55e9f-46d7-4285-a86f-c4994708a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffd043-65b1-4323-9672-171dd948d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During forward propagation, pre-activation and activation take place at each hidden and output layer node of a neural network. The pre-activation function is the calculation of the weighted sum. The activation function is applied, based on the weighted sum, to make the neural network flow non-linearly using bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b5c10-5e4a-4b88-9492-777389a6b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c89418-f8b6-4bfe-b291-f3292eeefec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means weight decide how fast the activation function will trigger whereas bias is used to delay the triggering of the activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b14d6-a0d4-4920-8455-e14901beaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07dc6f-8091-46cd-a410-6e71a7fcc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The softmax function, also known as the normalized exponential function, is a mathematical operation that maps arbitrary values into probabilities. It finds its place in neural networks as an activation function which can be used to ensure that the output of a node reflects probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07a30e-8dab-4227-842f-f085a466da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45050d-8177-41fe-bf51-4cf993322c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in a way that minimizes the loss by giving the nodes with higher error rates lower weights, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2676c2-9233-463c-9f8e-af7fa3bdbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df39509-151d-4c43-870b-778a2b73736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of backpropagation is to compute the gradient of the loss function concerning the weights and biases of each unit in the network. Then, we use the gradients obtained to update the parameters such that the loss we compute at the new value is less than the loss at the current value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6f8f8-e34b-4791-81bf-e67b5914e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a65cb7-8003-44d6-8fc7-c5165e666384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chain rule can be generalised to multivariate functions, and represented by a tree diagram. The chain rule is applied extensively by the backpropagation algorithm in order to calculate the error gradient of the loss function with respect to each weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6704e-b0b0-4848-97e6-aa9f15c9968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66eb741-b877-48c0-963e-140b0b7b23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanishing or exopliding gradients\n",
    "# incorrect weight updates\n",
    "# incorrect network architecture\n",
    "# overfitting or underfitting\n",
    "# data preprocessing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633cd14-a268-4ce5-9165-fdce6139e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4c666-5078-4317-8592-966bfd151ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
