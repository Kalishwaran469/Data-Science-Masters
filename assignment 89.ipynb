{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314eac19-701e-4ed5-908a-1e6af5b91453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e887d-79eb-4d0f-9d53-891bc70dea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d56c8-d2e8-4842-91a8-5ecbd0ddf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76025013-151d-431b-80c5-aea488d376e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear or Identity Activation Function.\n",
    "# Non-linear Activation Function.\n",
    "# Sigmoid or Logistic Activation Function.\n",
    "# Tanh or hyperbolic tangent Activation Function.\n",
    "# ReLU (Rectified Linear Unit) Activation Function.\n",
    "# Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc57fe3-47b6-4c7f-91de-c87361591c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20048f06-f8c0-4105-8a93-a67b95c93dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function helps in better training, learning process and, better generalizing capability. It controls the activation of every unit in the layer by working on weight and bias sum. This process incorporates the non-linearity in output of any neuron. The weights are updated based on the error in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323fe5b-c2fe-4b01-b4c3-7e357746f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa3e12-580e-4a86-8aee-2b04535dbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function is known as the logistic function which helps to normalize the output of any input in the range between 0 to 1. The main purpose of the activation function is to maintain the output or predicted value in the particular range, which makes the good efficiency and accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae746ebd-05ea-428f-a05a-9598aadd5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04eafc4-8d1c-409e-a338-f54a25362d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU boasts of training complex models/neural networks with constant values compared to sigmoid. Relu exhibits better efficiency: An advantage of ReLU, despite avoiding the vanishing gradients problem, is that it has a much lower run time max(0,f) and runs much faster than any sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c0293-28e9-412f-bca4-5de51460635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4b470-b2c8-4a7b-b971-89ca0e9546dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu : More computationally efficient to compute than Sigmoid like functions since Relu just needs to pick max(0, x) and not perform expensive exponential operations as in Sigmoids. Relu : In practice, networks with Relu tend to show better convergence performance than sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c8594-b411-406b-bdbd-8d0f196390c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806b4a6-8d92-4fe3-8c1a-b7cf6a3449c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU is known for vanishing gradients, since any values less than zero are mapped to zero. This is true regardless of the number of layers. LeakyReLU on the other hand, maps the values less than zeros to a very small positive number. This prevents vanishing gradient from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00fbb3-4feb-4fbe-91cf-f4c1837fc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4f185-b6de-4c06-b997-e1cf918eb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef6f0d-0fd0-4fe2-aa7d-d8fbad8dd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67faec-c1c1-43e9-b533-8af27e8ab766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1. Apparently the tanh function provides stronger gradients. Generally speaking, tanh has two main advantages over a sigmoid function: It has a slightly bigger derivative than the sigmoid (at least for the area around 0), which helps it to cope a bit better with the “vanishing gradients” problem of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52fa7a-fdc9-4e97-9c65-54b33bc96239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821450b-84a8-43ae-bbaf-0855f91cbdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573ce6a-8854-4179-8ee3-4adebb8bd004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
