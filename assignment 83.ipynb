{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c1959-b117-40c3-b388-193d84b7ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2920423-3a5b-4d07-9ce4-682dc421b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7dcd4-ad6d-4590-9bfe-27d80fd69b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055381d-67fc-491a-9b2a-8ad513e6f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application and measuring how much the application improves. It is an end-to-end evaluation where we can understand if a particular improvement in a component is really going to help the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231bfcb-6013-4b02-8e6c-278e82612c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720570e-c81d-4536-a57c-3bba2c49e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other NLP systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ef446-8d7b-48ca-978e-7c3bb77b3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960dfbc-ad37-4250-aebf-acc2d88f759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model. The matrix is populated with the predicted values from the model and the actual values from the test data. The matrix can be used to calculate a variety of metrics, such as accuracy, precision, recall, and specificity.A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect per class. It helps in understanding the classes that are being confused by model as other class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337455db-a2af-46a0-becf-c199c1d3d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5077f30-014e-4390-9c19-50ba08068168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation 1: Imbalanced Classes. Many categorization issues in the actual world have unequal distributions of the classes. ...\n",
    "# Limitation 2: Misclassification Costs. ...\n",
    "# Limitation 3: Probability Predictions. ...\n",
    "# Limitation 4: Ambiguity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
