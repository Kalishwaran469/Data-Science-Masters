{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94550894-eb80-4887-8ab7-37d4c508ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2afc1-b416-4683-b105-919621a3cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit).R 2 = 1 − sum squared regression (SSR) total sum of squares (SST) , = 1 − ∑ ( y i − y i ^ ) 2 ∑ ( y i − y ¯ ) 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f79deb-5d22-438d-a7a1-a0dfbecbaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cfeb3-3cd4-47c9-87ab-e59df38c0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90592632-4ad5-4c0f-8306-42fc5776dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d39932-d24a-4dee-bec4-bc110aafb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03579d-2efa-4c3b-9b89-45833d48a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acf07a-0673-4610-92ee-80d3150ddd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE is the square root of MSE. MSE is measured in units that are the square of the target variable, while RMSE is measured in the same units as the target variable. Due to its formulation, MSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a regression, taking the average over all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8535db-4e58-446d-9839-35c874a4f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7ca18-b5e5-4089-8c22-6340a13098de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function. Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "# MSE is highly biased for higher values. RMSE is better in terms of reflecting performance when dealing with large error values. RMSE is more useful when lower residual values are preferred.One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly. RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b4af3-0c30-4979-88eb-6eeb9d0b34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9ff0c-bee1-413a-a9c6-256bde6dfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09924812-507d-4e32-8b27-0927487134a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24b952-b726-49ff-8bf7-9a9d1d0b5305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c4915-3da7-4b88-8978-3c8a03bef8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b81a28-5514-470e-9c5b-6672a982f846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d04582-746b-416e-b072-e31a5f11ae91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
