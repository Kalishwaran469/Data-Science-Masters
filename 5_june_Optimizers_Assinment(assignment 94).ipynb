{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc9f97-0f0b-4a64-9849-0a22e9a1e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ff98f-50cd-44b1-a745-9e290304fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b2a76-7f5a-4846-b1ca-62a23f0a51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09029d81-3f49-4cf0-a250-76e189c2fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient descent algorithm is an optimization algorithm mostly used in machine learning and deep learning. Gradient descent adjusts parameters to minimize particular functions to local minima. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter's update (learning step).Batch gradient descent takes longer to converge since it computes the gradient using the entire training dataset in each iteration. Stochastic gradient descent, on the other hand, can converge faster since it updates the model parameters after processing each example, which can lead to faster convergence.Batch gradient descent requires more computation and memory since it needs to process the entire training dataset in each iteration. Stochastic gradient descent, on the other hand, requires less computation and memory since it only needs to process a single example or a small subset of examples in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b0874-8ca5-4c39-a5da-7f7dfe662266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc82d7-aa99-4024-8b2f-e4515f9c0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Minima and Saddle Point: For convex problems, gradient descent can find the global minimum easily, while for non-convex problems, it is sometimes difficult to find the global minimum, where the machine learning models achieve the best results. ...\n",
    "# Vanishing and Exploding Gradient.\n",
    "# The stochastic gradient (SG) algorithm behaves like a simulated annealing (SA) algorithm, where the learning rate of the SG is related to the temperature of SA. The randomness or noise introduced by SG allows to escape from local minima to reach a better minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb1ed6-87d3-4256-baf2-4afd622fc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e3f1c-eebd-4406-b90a-1cb1bcfa2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum can accelerate training and learning rate schedules can help to converge the optimization process. Adaptive learning rates can accelerate training and alleviate some of the pressure of choosing a learning rate and learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588424c-2102-4922-bbc4-f0d62e5bf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec18c8-5e13-47f6-990f-6f2774c8600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The advantage of using SGD is its computational efficiency, especially when dealing with large datasets. By using a single example or a small batch, the computational cost per iteration is significantly reduced compared to traditional Gradient Descent methods that require processing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e9870-187b-43c4-961a-af223e1fba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50506ae-8b63-4447-ba79-c59c11594980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In simple terms, Adam uses a combination of adaptive learning rates and momentum to make adjustments to the network's parameters during training. This helps the neural network learn faster and converge more quickly towards the optimal set of parameters that minimize the cost or loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe1b32-45a7-466d-a8a2-4a86bd6c1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c5069-ec5d-424a-9e85-baca3153cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp, root mean squared propagation is the optimization machine learning algorithm to train the Artificial Neural Network (ANN) by different adaptive learning rate and derived from the concepts of gradients descent and RProp.Furthermore, each dimension of the gradient is rescaled similar to RMSprop. One key difference between Adam and RMSprop (or AdaGrad) is the fact that the moment estimates m and v are corrected for their bias towards zero. Adam is well-known for achieving good performance with little hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001a525-f158-46f6-8db3-d7ce8f056274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054b38ff-f919-4e5a-a721-af959465ef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 06:50:42.285082: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-08 06:50:42.358621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-08 06:50:42.359814: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-08 06:50:43.569240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb742931-f1ac-4b67-9f14-399a5409680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0 werkzeug-2.3.7 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef74a979-5a44-4cbd-b13c-89c9271ca89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU isn't available!\n",
      "CPU is available!\n",
      "Details >> [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "check_list = ['GPU','CPU']\n",
    "\n",
    "for device in check_list:\n",
    "  out = tf.config.list_physical_devices(device)\n",
    "  if len(out) > 0:\n",
    "    print(f\"{device} is available!\")\n",
    "    print(f\"Details >> {out}\")\n",
    "  else:\n",
    "    print(f\"{device} isn't available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cc8eba-1cde-4a95-a6ef-0e8474bb74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5204cf49-1514-4353-8278-64466dabb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a94eb8-1e64-4168-b3b6-e78c4c332550",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
    "          tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\"),\n",
    "          tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\"),\n",
    "          tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
    "\n",
    "model_clf = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8e5d46-4ee4-4d92-b48b-5f1c088709e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputLayer (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " hiddenLayer1 (Dense)        (None, 300)               235500    \n",
      "                                                                 \n",
      " hiddenLayer2 (Dense)        (None, 100)               30100     \n",
      "                                                                 \n",
      " outputLayer (Dense)         (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57773c3-63d3-4a3f-9932-f3955b5a1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" \n",
    "sgd = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0) \n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=sgd,\n",
    "              metrics=METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad33947b-660b-4a59-a8a0-b89189b689e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6245 - accuracy: 0.8391 - val_loss: 0.3162 - val_accuracy: 0.9108\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2964 - accuracy: 0.9158 - val_loss: 0.2508 - val_accuracy: 0.9288\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2458 - accuracy: 0.9295 - val_loss: 0.2134 - val_accuracy: 0.9414\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2116 - accuracy: 0.9401 - val_loss: 0.1901 - val_accuracy: 0.9468\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1848 - accuracy: 0.9469 - val_loss: 0.1673 - val_accuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history = model_clf.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                    validation_data=VALIDATION_SET, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6005c1f7-36ab-44c1-a649-4f29b52b6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" \n",
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=adam,\n",
    "              metrics=METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f940a98-47de-46a2-a720-5ea5b659d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1542 - accuracy: 0.9531 - val_loss: 0.1294 - val_accuracy: 0.9596\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0796 - accuracy: 0.9753 - val_loss: 0.0883 - val_accuracy: 0.9746\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0594 - accuracy: 0.9814 - val_loss: 0.0818 - val_accuracy: 0.9752\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0421 - accuracy: 0.9861 - val_loss: 0.0837 - val_accuracy: 0.9742\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0337 - accuracy: 0.9893 - val_loss: 0.0907 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history = model_clf.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                    validation_data=VALIDATION_SET, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549c09c2-7157-4df5-9c83-fd31f26fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" \n",
    "rmsprop = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.1,\n",
    "    rho=0.9,\n",
    "    momentum=0.0)\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "model_clf.compile(loss=LOSS_FUNCTION,\n",
    "              optimizer=rmsprop,\n",
    "              metrics=METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0052d0-e608-422d-9c61-66171e4882fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 8.1823 - accuracy: 0.2857 - val_loss: 1.9077 - val_accuracy: 0.2874\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.2153 - accuracy: 0.2669 - val_loss: 1.7905 - val_accuracy: 0.3014\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.0934 - accuracy: 0.2807 - val_loss: 2.0252 - val_accuracy: 0.2214\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.1070 - accuracy: 0.2602 - val_loss: 1.9813 - val_accuracy: 0.3326\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 2.1854 - accuracy: 0.2965 - val_loss: 1.8612 - val_accuracy: 0.2832\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "VALIDATION_SET = (X_valid, y_valid)\n",
    "\n",
    "history = model_clf.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                    validation_data=VALIDATION_SET, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d0b9ff-b63e-4e9d-afce-7da9fd039c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d55ba-91df-41a7-b25f-27520082dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider Your Data Set. One of the first considerations to work through is your training data set. An optimizer that does well with one type of data set may not be sufficient for another. Some of the most important factors in this area to keep in mind are your information's size and how varied it is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
