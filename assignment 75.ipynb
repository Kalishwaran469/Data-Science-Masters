{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7ee20-8da3-4517-9a02-e51b50b90f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5112a-bd8a-4859-afec-942c2902a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add dimensions (features), the minimum data requirements also increase rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c04779-8845-4860-8cf4-e85bf4d90922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8711a20-cb81-4404-a2cf-b38d7d972853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in curse of dimensionality we can't able to visualise the data .As the dimensionality increases, the number of data points required for good performance of any machine learning algorithm increases exponentially. The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9212f-0b58-446c-a469-0d5d4a6f6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8c993-895a-4b4e-ae0b-ae3e06fd5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The higher the dimensions, the more difficult it will be to sample from because the sampling loses its randomness. It becomes harder to collect observations if there are plenty of features. These dimensions make all observations in the dataset to be equidistant from all other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df830f-aeab-4f4f-83a1-cdcd3ead00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b657b4c-c130-4c3d-9847-8f069ae697c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection is a technique used to select most important features in the dataset.for eg:in a dataset there are 2 independent features but 1 features is not important to predict the output so in the case we only select other important feature to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb47e14-e7cb-4788-807b-13a0432658e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a49b3-0565-41d9-a974-56518a23f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "# It may need a lot of processing power.\n",
    "# Interpreting transformed characteristics might be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2f4f9-cf3f-4b43-982d-4aeb360f99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e8e82-f704-4499-8a3a-d45f9bad636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of this inherent sparsity we end up overfitting, when we add more features to our data, which means we need more data to avoid sparsity â€” and that's the curse of dimensionality: as the number of features increase, our data become sparser, which results in overfitting, and we therefore need more data to avoid it.The idem curse of dimensionality may suggest that we keep our models simple, but on the other hand, if our model is too simple we run the risk of suffering from underfitting. Underfitting problems arise when our model has such a low representation power that it cannot model the data even if we had all the training data we want. We clearly have underfitting when our algorithm cannot achieve good performance measures even when measuring on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82419e9e-331e-4d19-bd41-db9354052a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc41558-4e37-400f-b12c-90cbb501ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only we determine the optimal number of dimensions among 1 or 2 or 3 because generally we as a human can abke to visualise the data in 1d,2d and 3d visualisation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77d1c2-fc7c-464f-9ad2-59f664f4354a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
