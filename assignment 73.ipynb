{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0fcae-1c06-467d-8a2e-0bf6de0b3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c43009-452a-4af1-8d42-900235fec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main difference between euclidean distance and manhattan distance is euclidean distance is calculated the shortest path between source and destination ln a straight line but on the other hand manhattan distance is calculated the sum of all the real distances between source and destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c587cc-4321-4847-b02f-fd438392a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e86feb-bc8f-4132-8c9f-4b1aad37cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d4cb0-8d88-43ac-90b2-60a171cd6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37992fa7-bd56-471c-aa1c-7b369a5d5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We mean by the 'best distance metric' (in this review) is the one that allows the KNN to classify test examples with the highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.datapoints are in a straight line we use euclidean but datapoints are in a not straight line we might use manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb0177-81d2-4368-a890-bf1802f5085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285794ac-9aad-404b-a21f-e4ed38ed6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the common hyperparameters in knn classifier and regressor are n_neighbors and algotithm.n_neighbors is a k value it defines how many knn datas we need to select also algorithm is a other important parametric defined which tree we need to select for split the data.here we can improve the performance of the model by always assign a odd value to n_neighbors because k value is even sometimes in classification problems when classify the data it ties so we must use odd value for n_neighbors also we majorly need to use ball tree algorithm because it is a simple tree method to split the data compared with kd tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafba563-0940-447f-987f-bbe0a4bb305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9260c0-4313-48e4-bda4-1b981b047f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in the dataset which adversely affect the performance of KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533aa7a3-1839-43d5-a919-8d7348695af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b050ce-4fcb-4f84-94dc-bafdaec372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the outliers before applying the KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f84dc2-e90a-439b-8e5c-1e5e7024cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb601468-2ba4-46ee-9d82-9a96af5f1af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54770269-f40e-450a-a7e1-77fd5d3960f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
